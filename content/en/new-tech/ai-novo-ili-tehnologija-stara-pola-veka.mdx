---
title: "AI – something new or a fifty-year-old technology?"
description: "A brief history of artificial intelligence, from the Turing test to ChatGPT – and why AI seems to have exploded overnight."
date: "2025-11-15"
category: "new-tech"
image: "/images/new-tech/AI-human.jpg"
author: "InfoHelm Team"
hero: "/images/new-tech/AI-human.jpg"
heroAlt: "Human and robotic hands shaking in a digital environment"
tags:
  - "artificial intelligence"
  - "AI"
  - "generative AI"
  - "history of technology"
---

# AI – something new or a fifty-year-old technology?

Suddenly everyone is talking about AI.  
On front pages, in phone commercials, in writing tools, in photo apps… If we believed the media, artificial intelligence **“came out of nowhere” around 2022/2023.**

But if we scratch the surface a bit, one uncomfortable question appears:

> Is AI really a brand-new thing – or **a technology that has been here for 50+ years**, only now released “to ordinary people”?

![Humans and AI – collaboration, not just rivalry](/images/new-tech/AI-human.jpg)

---

## Why does it look like AI exploded “overnight”?

In just a few years, the number of people using generative AI tools jumped from a **niche of geeks** to **hundreds of millions of users**. The chart makes it obvious:

![Global growth of generative AI tool users 2022–2025](/images/new-tech/ai_tools_users_growth_2022_2025_dark.png)

*Chart: approximate growth of monthly active users of generative AI tools – from tens of millions to around a billion users between 2022 and 2025.*

From an average user’s point of view, the story looks like this:

- yesterday nothing existed,
- today we have ChatGPT, image generators, music, code, video,
- tomorrow – who knows, maybe fully autonomous systems.

However, under the surface this “explosion” is the result of a **long development history** plus a few key technological and business turning points.

---

## A short history of artificial intelligence (without textbooks)

### 1950s: Turing and the question “can machines think?”

Back in 1950 Alan Turing asks the famous question:  
**“Can machines think?”** and proposes the Turing test as a practical criterion.

The idea of artificial intelligence is more philosophical than practical at that time – but the seed is planted.

### 1960s and 1970s: symbolic AI and expert systems

In the following decades, so-called **symbolic AI** dominates:

- “if–then” rules,
- logical systems,
- early **expert systems** helping, for example, doctors with diagnosis.

Computers are slow, data is limited, but ambition is huge:  
encode *knowledge* directly into the system via rules.

### 1980s: neural networks and backpropagation

In the 1980s interest in **neural networks** returns – mathematical models inspired by neurons in the brain.

The key moment: the **backpropagation** method, which allows networks to be trained more efficiently.

Still, we’re missing:

- powerful enough processors,
- huge datasets,
- and “cheap” computing infrastructure.

### 1990s and 2000s: data mining and early machine learning

With the growth of the internet and databases, the focus shifts to:

- **data mining** – digging through large datasets,
- **classification, regression, recommendations**,
- early *machine learning* algorithms (SVM, random forest, boosting…).

AI is already quietly working in the background:  
in search engines, movie recommendations, spam filters, credit scoring systems.

---

## AI in the military and “secret” projects

The technologies we now call AI have long been used **behind closed doors**:

- radar and sonar systems,
- satellite image analysis,
- automatic signal recognition and cryptography,
- early forms of speech and image recognition.

The logic is simple:

1. if a technology gives a strategic advantage,  
2. the military and intelligence agencies will use it first,  
3. only then does it reach the civilian sector.

That doesn’t mean there is one “secret mega-mind”, but it’s realistic to assume that:

- experiments with **autonomous drones**,  
- systems for **mass data analytics**,  
- and advanced predictive models

started **long before** we saw the first public chatbots.

---

## Why did the “explosion” happen only now?

Three main reasons why we got **massive, visible** AI only in the 2020s:

### 1. The GPU revolution

Graphics cards (GPUs) built for gaming turned out to be perfect for:

- parallel computation,
- training large neural networks.

With the arrival of **cloud platforms** (AWS, GCP, Azure) access to that power became *rentable* – you don’t need your own data center anymore.

### 2. Big data – the internet as a huge dataset

The internet has provided:

- billions of sentences of text,
- billions of images, videos, code snippets,
- endless logs of user behaviour.

Modern models are trained exactly on those datasets.

### 3. The business model

Only now is there a clear **business incentive**:

- automating support and operations,
- speeding up development,
- a new generation of SaaS tools,
- a race between Big Tech players for market share.

The result: AI is no longer hiding “behind the scenes” – it’s being **pushed straight into users’ hands**.

---

## How today’s generative models differ from “old” AI

These days we mostly talk about **generative AI** – models that write text, draw images, compose music and write code.

At a high level, the difference looks like this:

- **Expert systems (old AI)** – hand-coded rules, “if–then” logic, narrow domains.
- **LLMs and deep learning (new AI)** – learn from massive amounts of data, spotting statistical patterns.

One way to visualize where users are today is by types of AI tools they use:

![Generative AI tools by type – estimated users](/images/new-tech/ai_tools_by_type_stacked_2022_2025_dark.png)

*Chart: approximate distribution of users of generative AI tools by category – chatbots, image generators, code assistants and other AI tools.*

### “Parrots” that still get the job done

It’s often said that LLMs are just **“sophisticated parrots”**:

- they don’t understand the world like humans do,
- they predict the next word based on probability.

But in practice:

- that “parrot” can **summarise hundreds of pages**,  
- structure information,
- write a first draft of a text, code or report.

In other words: it may not be a conscious being, but it is **a very useful tool**.

---

## What probably already exists behind the scenes

If models this powerful are available publicly, it’s reasonable to assume that:

- there are larger, specialised versions for analytics,
- AI is already widely used in **intelligence analysis, financial markets, cyber security**,  
- **more autonomous systems** are being tested than we currently see in civilian apps.

The historical pattern is similar:

1. technology is born in labs and special projects,  
2. it is used quietly in the background,  
3. only then does it reach the **consumer level** as a “new thing”.

---

## Risks and opportunities

### What can be good?

The positive side is significant:

- **Medicine** – faster image analysis, personalised therapy, pattern detection in medical data.
- **Science** – generating hypotheses, analysing complex systems, accelerating research.
- **Creativity** – tools for music, images, video, writing; a single person with a laptop gains the power of a small studio.

### What can go wrong?

Serious risks also exist:

- **Deepfakes and manipulation** – it becomes harder and harder to distinguish truth from simulation.
- **Surveillance** – combining cameras, face recognition and analytics creates new forms of tracking.
- **Job automation** – jobs don’t “disappear overnight”, but they are **reshaped**, often faster than education systems can adapt.

---

## Three short scenarios

1. **Optimistic** – AI becomes a “co-pilot” at work and in everyday life; productivity rises, new kinds of jobs appear, medical research accelerates.
2. **Pessimistic** – surveillance, manipulation, concentration of power in the hands of a few corporations and states; social inequalities deepen.
3. **Realistic mix** – we get both; AI becomes a *mirror of humanity*: it amplifies both the good and the bad tendencies.

---

## Conclusion: AI as a mirror, not magic

Artificial intelligence is not magic that “fell from the sky” in 2022.  
It is the result of **decades of work** on mathematics, electronics, software – and human ambition.

> AI doesn’t just change technology; it speeds up what people **already are**:
> creativity and greed, empathy and manipulation.

That’s why the most important question for the next 10–20 years is less technical and more human:

- not just *“what can AI do”*,  
- but *“what do we want to do with AI”*.

Hype and fear come and go, but responsibility stays – with us.
